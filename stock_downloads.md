For **AdaBins** [pretrained model](https://drive.google.com/drive/folders/1nYyaQXOBjNdUJDsmJpcRpu6oE55aQoLA?usp=sharing) is avaliable.

For **FastDepth** next cases are available:

* [pretrained with ImageNet model](http://datasets.lids.mit.edu/fastdepth/imagenet/results/imagenet.arch=mobilenet.lr=0.1.bs=256/model_best.pth.tar)

* [MobileNet-NNConv5](http://datasets.lids.mit.edu/fastdepth/results/mobilenet-nnconv5.pth.tar)

* [MobileNet-NNConv5(depthwise)](http://datasets.lids.mit.edu/fastdepth/results/mobilenet-nnconv5dw.pth.tar)

* [MobileNet-NNConv5(depthwise), with additive skip connections](http://datasets.lids.mit.edu/fastdepth/results/mobilenet-nnconv5dw-skipadd.pth.tar)

* [MobileNet-NNConv5(depthwise), with additive skip connections pruned](http://datasets.lids.mit.edu/fastdepth/results/mobilenet-nnconv5dw-skipadd-pruned.pth.tar)

For **DORN** [pretrained backbone](http://sceneparsing.csail.mit.edu/model/pretrained_resnet/resnet101-imagenet.pth) is available.

For **FeatDepth** next cases are available:

* [AutoEncoder trained on the kitti raw data](https://drive.google.com/file/d/1ncAWUMvLq2ETMpG-7eI9qfILce_cPPfy/view?usp=sharing)

* [FeatDepth trained on the kitti raw data](https://drive.google.com/file/d/1HlAubfuja5nBKpfNU3fQs-3m3Zaiu9RI/view?usp=sharing)

* [FeatDepth finetuned on the test split of kitti raw data by using online refinement](https://drive.google.com/file/d/1CfCtz55s4QHya3y3UslxsuD_0cxNlA-D/view?usp=sharing)

* [FeatDepth trained on kitti odometry](https://drive.google.com/file/d/1vQJbiyPXv_XNQYpyVocDB3-LKwx2LVka/view?usp=sharing)

For **BTS** next cases are available:

* [ResNet50](https://cogaplex-bts.s3.ap-northeast-2.amazonaws.com/bts_eigen_v2_pytorch_resnet50.zip)

* [Resnet101](https://cogaplex-bts.s3.ap-northeast-2.amazonaws.com/bts_eigen_v2_pytorch_resnet101.zip)

* [ResNext50](https://cogaplex-bts.s3.ap-northeast-2.amazonaws.com/bts_eigen_v2_pytorch_resnext50.zip)

* [ResNext101](https://cogaplex-bts.s3.ap-northeast-2.amazonaws.com/bts_eigen_v2_pytorch_resnext101.zip)

* [DenseNet121](https://cogaplex-bts.s3.ap-northeast-2.amazonaws.com/bts_eigen_v2_pytorch_densenet121.zip)

* [DenseNet161](https://cogaplex-bts.s3.ap-northeast-2.amazonaws.com/bts_eigen_v2_pytorch_densenet161.zip)

For **MonoDepth2** next cases are available:

* [mono_640x192](https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_640x192.zip)

* [stereo_640x192](https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_640x192.zip)

* [mono+stereo_640x192](https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_640x192.zip)

* [mono_1024x320](https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_1024x320.zip)

* [stereo_1024x320](https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_1024x320.zip)

* [mono+stereo_1024x320] (https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_1024x320.zip)

* [mono_no_pt_640x192](https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_no_pt_640x192.zip)

* [stereo_no_pt_640x192](https://storage.googleapis.com/niantic-lon-static/research/monodepth2/stereo_no_pt_640x192.zip)

* [mono+stereo_no_pt_640x192](https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_no_pt_640x192.zip)

* [mono_odom_640x192](https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_odom_640x192.zip)

* [mono+stereo_odom_640x192](https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono%2Bstereo_odom_640x192.zip)

* [mono_resnet50_640x192](https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_resnet50_640x192.zip)

* [mono_resnet50_no_pt_640x192](https://storage.googleapis.com/niantic-lon-static/research/monodepth2/mono_resnet50_no_pt_640x192.zip)
