{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example prediction on a single test image\n",
    "\n",
    "This notebook gives example code to make a single disparity prediction for one test image.\n",
    "\n",
    "The file `test_simple.py` shows a more complete version of this code, which additionally:\n",
    "- Can run on GPU or CPU (this notebook only runs on CPU)\n",
    "- Can predict for a whole folder of images, not just a single image\n",
    "- Saves predictions to `.npy` files and disparity images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image as pil\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up network and loading weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"evo_scratch\"\n",
    "model_path = '/media/data/datasets/penitto/networks/monodepth2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_path = os.path.join(model_path, model_name, \"models\", \"weights_28\", \"encoder.pth\")\n",
    "depth_decoder_path = os.path.join(\n",
    "    model_path, model_name, \"models\", \"weights_28\", \"depth.pth\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = networks.ResnetEncoder(50, False)\n",
    "depth_decoder = networks.DepthDecoder(num_ch_enc=encoder.num_ch_enc, scales=range(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING PRETRAINED MODEL\n",
    "\n",
    "\n",
    "loaded_dict_enc = torch.load(encoder_path, map_location='cpu')\n",
    "filtered_dict_enc = {\n",
    "    k: v for k, v in loaded_dict_enc.items() if k in encoder.state_dict()\n",
    "}\n",
    "encoder.load_state_dict(filtered_dict_enc)\n",
    "\n",
    "loaded_dict = torch.load(depth_decoder_path, map_location='cpu')\n",
    "depth_decoder.load_state_dict(loaded_dict)\n",
    "\n",
    "encoder.eval()\n",
    "depth_decoder.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "25557032 + 9014100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in encoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.numel() for p in depth_decoder.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readlines(filename):\n",
    "    \"\"\"Read all the lines in a text file and return as a list\"\"\"\n",
    "    with open(filename, 'r') as f:\n",
    "        lines = f.read().splitlines()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst = readlines('/home/penitto/mono_depth/eval_imgs/ev_img.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disp_to_depth(disp, min_depth, max_depth):\n",
    "    \"\"\"Convert network's sigmoid output into depth prediction\n",
    "    The formula for this conversion is given in the 'additional considerations'\n",
    "    section of the paper.\n",
    "    \"\"\"\n",
    "    min_disp = 1 / max_depth\n",
    "    max_disp = 1 / min_depth\n",
    "    scaled_disp = min_disp + (max_disp - min_disp) * disp\n",
    "    depth = 1 / scaled_disp\n",
    "    return scaled_disp, depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_image(x):\n",
    "    \"\"\"Rescale image pixels to span range [0, 1]\"\"\"\n",
    "    ma = float(x.max())\n",
    "    mi = float(x.min())\n",
    "    d = ma - mi if ma != mi else 1e5\n",
    "    return (x - mi) / d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lst[:1]:\n",
    "    split = i.split()\n",
    "    image_path = (\n",
    "        \"/home/penitto/mono_depth/eval_imgs/\" + split[0] + '_' + split[1] + '.jpg'\n",
    "    )\n",
    "\n",
    "    input_image = pil.open(image_path).convert('RGB')\n",
    "    original_width, original_height = input_image.size\n",
    "\n",
    "    feed_height = loaded_dict_enc['height']\n",
    "    feed_width = loaded_dict_enc['width']\n",
    "    input_image_resized = input_image.resize((feed_width, feed_height), pil.LANCZOS)\n",
    "\n",
    "    input_image_pytorch = transforms.ToTensor()(input_image_resized).unsqueeze(0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        features = encoder(input_image_pytorch)\n",
    "        outputs = depth_decoder(features)\n",
    "\n",
    "    disp = outputs[(\"disp\", 0)]\n",
    "\n",
    "    new_path = os.path.splitext(image_path)[0]\n",
    "    new_path += '_depth.png'\n",
    "    new_path\n",
    "\n",
    "    disp_resized = (\n",
    "        torch.nn.functional.interpolate(\n",
    "            disp, (original_height, original_width), mode=\"bilinear\", align_corners=False\n",
    "        )\n",
    "        .squeeze()\n",
    "        .cpu()\n",
    "        .numpy()\n",
    "    )\n",
    "    #     print(original_height, original_width)\n",
    "    #     print(disp.shape)\n",
    "    #     print(type(disp))\n",
    "\n",
    "    print(normalize_image(disp))\n",
    "    # Saving colormapped depth image\n",
    "    #     disp_resized_np = np.clip(disp_to_depth(disp_resized.squeeze().cpu().numpy(), 2, 117)[1], 2, 117)\n",
    "    # vmax = np.percentile(disp_resized_np, 95)\n",
    "    s = 120 * 2.12 * 1000000\n",
    "    disp_resized_np = 1 / normalize_image(disp_resized)\n",
    "    vmax = np.percentile(disp_resized_np, 99)\n",
    "    vmin = np.percentile(disp_resized_np, 5)\n",
    "\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.subplot(211)\n",
    "    plt.imshow(input_image)\n",
    "    plt.title(\"Input\", fontsize=22)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(212)\n",
    "    #     plt.imsave(new_path, disp_resized_np, cmap='magma', vmax=vmax)\n",
    "    plt.imshow(disp_resized_np, cmap='magma', vmax=vmax)\n",
    "    plt.title(\"Disparity prediction\", fontsize=22)\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction using the PyTorch model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imread(\n",
    "    '../../eval_imgs/ckad_01_ckad_2020-10-29-17-01-56_0_1603980116931038171.jpg'\n",
    ").shape[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in lst:\n",
    "    split = i.split()\n",
    "    image_path = (\n",
    "        \"/home/penitto/mono_depth/eval_imgs/\" + split[0] + '_' + split[1] + '.png'\n",
    "    )\n",
    "    save_path = (\n",
    "        \"/home/penitto/mono_depth/eval_imgs/\" + split[0] + '_' + split[1] + '_alt.png'\n",
    "    )\n",
    "    #     gt_depth = \"/home/penitto/mono_depth/eval_imgs/ckad_01_ckad_2020-10-29-17-01-56_0_1603980116931038171.png\"\n",
    "    input_image = pil.open(image_path)\n",
    "    vmax = np.percentile(input_image, 95)\n",
    "    plt.imsave(save_path, input_image, cmap='magma', vmax=vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmax = np.percentile(input_image, 95)\n",
    "plt.imsave('/home/penitto/s.png', input_image, cmap='magma', vmax=vmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
